<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      -->
<title>Double dipping</title>
<meta name="generator" content="MATLAB 24.1">
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
<meta name="DC.date" content="2025-10-25">
<meta name="DC.source" content="run_bad_double_dipping_analysis.m">
<style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style>
</head>
<body>
<div class="content">
<h1>Double dipping</h1>
<pre class="codeinput">
<span class="comment">% Warning: this exercise shows the *bad* practice of double dipping</span>
<span class="comment">% (also known as circular analysis). You must never, ever use</span>
<span class="comment">% results double dipping to interpret results for a real analysis that you</span>
<span class="comment">% would publish.</span>

nfeatures = 100;
nsamples_per_class = 200;
nclasses = 2;
niter = 1000;

<span class="comment">% compute number of samples</span>
nsamples = nclasses * nsamples_per_class;

<span class="comment">% set targets</span>
targets = repmat((1:nclasses)', nsamples_per_class, 1);

<span class="comment">% allocate space for output</span>
accuracies = zeros(niter, 2);

<span class="keyword">for</span> iter = 1:niter
    <span class="comment">% generate random gaussian train data of size nsamples x nfeatures</span>
    <span class="comment">% assign the result to a variable 'train_data'</span>
    <span class="comment">% &gt;@@&gt;</span>
    train_data = randn(nsamples, nfeatures);
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% for the double dipping test data, assign 'double_dipping_test_data'</span>
    <span class="comment">% to be the same as the training data.</span>
    <span class="comment">%</span>
    <span class="comment">% *** WARNING ***</span>
    <span class="comment">% For real data analyses (that you would publish in a paper) you</span>
    <span class="comment">% must never do double dipping analysis - its results are invalid</span>
    <span class="comment">% ****************</span>
    <span class="comment">% &gt;@@&gt;</span>
    double_dipping_test_data = train_data;
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% for the independent data, generate random gaussian data (of the</span>
    <span class="comment">% same size as train_data) and assign to a variable</span>
    <span class="comment">% 'independent_test_data'</span>
    <span class="comment">% &gt;@@&gt;</span>
    independent_test_data = randn(nsamples, nfeatures);
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% compute class labels predictions for both test sets using</span>
    <span class="comment">% cosmo_classify_lda. Store the predictions in</span>
    <span class="comment">% 'double_dipping_pred' and 'independent_pred', respectively</span>
    <span class="comment">% &gt;@@&gt;</span>
    double_dipping_pred = cosmo_classify_lda(train_data, targets, <span class="keyword">...</span>
                                             double_dipping_test_data);
    independent_pred = cosmo_classify_lda(train_data, targets, <span class="keyword">...</span>
                                          independent_test_data);
    <span class="comment">% &lt;@@&lt;</span>

    <span class="comment">% compute classification accuracies</span>
    double_dipping_acc = mean(double_dipping_pred == targets);
    independent_acc = mean(independent_pred == targets);

    <span class="comment">% store accuracies in the iter-th row of the 'accuracies' matrix</span>
    <span class="comment">% &gt;@@&gt;</span>
    accuracies(iter, :) = [double_dipping_acc, independent_acc];
    <span class="comment">% &lt;@@&lt;</span>
<span class="keyword">end</span>

<span class="comment">% show histogram</span>
hist(accuracies, 100);
legend({<span class="string">'double dipping'</span>, <span class="string">'independent'</span>});
</pre>
<img vspace="5" hspace="5" src="run_bad_double_dipping_analysis_01.png" alt=""> <p class="footer">
<br>
<a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2024a</a>
<br>
</p>
</div>
<!--
##### SOURCE BEGIN #####
%% Double dipping

% Warning: this exercise shows the *bad* practice of double dipping
% (also known as circular analysis). You must never, ever use
% results double dipping to interpret results for a real analysis that you
% would publish.

nfeatures = 100;
nsamples_per_class = 200;
nclasses = 2;
niter = 1000;

% compute number of samples
nsamples = nclasses * nsamples_per_class;

% set targets
targets = repmat((1:nclasses)', nsamples_per_class, 1);

% allocate space for output
accuracies = zeros(niter, 2);

for iter = 1:niter
    % generate random gaussian train data of size nsamples x nfeatures
    % assign the result to a variable 'train_data'
    % >@@>
    train_data = randn(nsamples, nfeatures);
    % <@@<

    % for the double dipping test data, assign 'double_dipping_test_data'
    % to be the same as the training data.
    %
    % *** WARNING ***
    % For real data analyses (that you would publish in a paper) you
    % must never do double dipping analysis - its results are invalid
    % ****************
    % >@@>
    double_dipping_test_data = train_data;
    % <@@<

    % for the independent data, generate random gaussian data (of the
    % same size as train_data) and assign to a variable
    % 'independent_test_data'
    % >@@>
    independent_test_data = randn(nsamples, nfeatures);
    % <@@<

    % compute class labels predictions for both test sets using
    % cosmo_classify_lda. Store the predictions in
    % 'double_dipping_pred' and 'independent_pred', respectively
    % >@@>
    double_dipping_pred = cosmo_classify_lda(train_data, targets, ...
                                             double_dipping_test_data);
    independent_pred = cosmo_classify_lda(train_data, targets, ...
                                          independent_test_data);
    % <@@<

    % compute classification accuracies
    double_dipping_acc = mean(double_dipping_pred == targets);
    independent_acc = mean(independent_pred == targets);

    % store accuracies in the iter-th row of the 'accuracies' matrix
    % >@@>
    accuracies(iter, :) = [double_dipping_acc, independent_acc];
    % <@@<
end

% show histogram
hist(accuracies, 100);
legend({'double dipping', 'independent'});

##### SOURCE END #####
-->
</body>
</html>
